# base
train_type: llm
seed: 0
deterministic: True
model: koalpaca
model_size: 10b     # valid pattern examples: 10b, 1.2b, 2.8, 5 (you can omit b character)

# data config
data_path: ['data/koalpaca_easy']

# project config
project: outputs/llm
name: llm_test

# environment config
device: [0]
bit: 8
half_inference: False

# data config
max_length: 256
add_eos_token_when_respose_end: True
data_verbose: False

# tokenizer config (I recommend to double check the tokenizer's specila token map)
pad_token_id: 0       # if null, tokenizer pad_token_id will not be overrided
bos_token_id: null    # if null, tokenizer bos_token_id will not be overrided
eos_token_id: 1       # if null, tokenizer eos_token_id will not be overrided
cls_token_id: null    # if null, tokenizer cls_token_id will not be overrided
sep_token_id: null    # if null, tokenizer sep_token_id will not be overrided
unk_token_id: null    # if null, tokenizer unk_token_id will not be overrided

# training config
batch_size: 4
epochs: 300
warmup_epochs: 0
steps: 15000
warmup_steps: 300
optimizer_step_criterion: 'step'   # ['epoch', 'step']
lr0: 5e-4
lrf: 0.001      # last_lr = lr0 * lrf
scheduler_type: 'cosine'   # ['linear', 'cosine']
momentum: 0.9
weight_decay: 0.0
warmup_momentum: 0.8
early_stop_criterion: 50
workers: 0
amp_training: False
ema_updating: False
train_verbose: True

# peft
peft_config_path: config/llm_lora_config.yaml   # if False, training without peft

# logging
log_data: ['train_loss', 'validation_loss', 'lr']  # ['train_loss', 'validation_loss', 'lr']