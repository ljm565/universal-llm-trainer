# base
seed: 0
deterministic: True
model: qwen
model_size: 7b     # valid pattern examples: 10b, 1.2b, 2.8, 5 (you can omit b character)

# data config
data_path: ['data/kopolyglot_easy_v2']
template_dir: ['templates/phi3_templates_ko']   # list(path) or path, if null, template will be chosen according to the {data_path}/templates

# project config
project: outputs/qwen
name: test

# environment config
device: [1]
bit: 4
quant_config: config_quant/bnb_4bit.yaml    # str, if you set bit as 4 or 8, you have to insert appropriate quantization config, or assertion error will occur. It only activate when you set bit to 4 or 8.
attn_implementation: null      # [flash_attention_2, sdpa, eager], if null, None default value will be applied.
load_unnecessary_half: False    # recommend to set True if you set bit to 16 (4, 8bit model automatically loads neccesaries to 32bit)
half_inference: False

# FSDP training configs
fsdp_train: False
fsdp_hyperparameters:
    cpu_offload: True
    amp_training: True
    wrap_policy: size_based     # [size_based, transformer_based]
    size_based:
        min_num_params: 1000000     # integer value
    transformer_based:
        fsdp_layer_cls: null    # List of modules to apply FSDP

# data config
max_length: 1500
is_multi_turn: False
add_bos_token_when_response_start: True
add_eos_token_when_response_end: True
data_verbose: False

# tokenizer config (I recommend to double check the tokenizer's special token map)
pad_token_id: 151850    # [add, null, int] if null, tokenizer pad_token_id will not be overrided
bos_token_id: 151848    # [add, null, int] if null, tokenizer bos_token_id will not be overrided
eos_token_id: 151849    # [add, null, int] if null, tokenizer eos_token_id will not be overrided
cls_token_id: null    # [add, null, int] if null, tokenizer cls_token_id will not be overrided
sep_token_id: null    # [add, null, int] if null, tokenizer sep_token_id will not be overrided
unk_token_id: null    # [add, null, int] if null, tokenizer unk_token_id will not be overrided

# training config
batch_size: 2
epochs: 300
warmup_epochs: 0
steps: 120000
warmup_steps: 100
optimizer_step_criterion: 'step'   # ['epoch', 'step']
lr0: 1e-4
lrf: 0.001      # last_lr = lr0 * lrf
scheduler_type: 'cosine'   # ['linear', 'cosine']
momentum: 0.9
weight_decay: 0.0
warmup_momentum: 0.8
early_stop_criterion: 50
workers: 0
total_cpu_use: 32
amp_training: False
ema_updating: False
train_verbose: True
inference_result_verbose: True
gradient_checkpointing: False    # if True, gradient checkpointing strategy will be used to save GPU memeory consuption.
gradient_accumuate_step: 1
generation_max_time: 180        # Maximum time (seconds) of model's generation function. If null, no time limits. 

# peft
peft_config_path: config_lora/llm_qwen_lora_config.yaml   # if False, training without peft

# logging data and metrics
common: ['train_loss', 'validation_loss', 'lr']               # ['train_loss', 'validation_loss', 'lr']
metrics: ['ppl', 'bleu', 'rouge', 'edit_distance']  # ['ppl', 'bleu', 'edit_distance', 'rouge', 'meteor']  # TODO: CIDEr 추가
fast_validation_n: null                                         # [null, 1, 2, ...], Only the number of data of the set value is evaluated per step
fast_validation_step_interval: 10                            # [null, 1, 2, ...], if null, all validation steps will be executed
validation_step_interval_prop: 0.3                            # setting between 0 and 1 values
tensorboard_logging_interval: 1